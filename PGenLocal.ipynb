{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import random\n",
    "import re\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Successfully imported all libraries And downloaded necessary data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_files):\n",
    "    text = \"\"\n",
    "    for pdf_file in pdf_files:\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    print(f\"Extracted text sample: {text[:200]}...\")  # Print a sample of the extracted text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove numbering and special characters\n",
    "    text = re.sub(r'\\d+\\.|\\(|\\)', '', text)\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        preprocessed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_questions(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    analyzed_questions = []\n",
    "    for sent in doc.sents:\n",
    "        if sent.text.strip().endswith('?'):\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            pos_tags = [token.pos_ for token in sent]\n",
    "            analyzed_questions.append({\n",
    "                'text': sent.text,\n",
    "                'entities': entities,\n",
    "                'pos_tags': pos_tags\n",
    "            })\n",
    "    \n",
    "    return analyzed_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topics(preprocessed_text):\n",
    "    # Create a dictionary from the preprocessed text\n",
    "    dictionary = corpora.Dictionary([text.split() for text in preprocessed_text])\n",
    "    \n",
    "    # Create a corpus\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in preprocessed_text]\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100)\n",
    "    \n",
    "    topics = lda_model.print_topics()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    filtered_questions = []\n",
    "    for question in generated_questions:\n",
    "        # Check if the question is similar to any original question\n",
    "        if any(question.lower() in orig['text'].lower() for orig in original_questions):\n",
    "            continue\n",
    "        \n",
    "        # Check if the question contains any topic keywords\n",
    "        if any(keyword in question.lower() for topic in topics for keyword in topic[1].split()):\n",
    "            filtered_questions.append(question)\n",
    "    \n",
    "    # If we have fewer than 5 questions, add some generated questions without filtering\n",
    "    if len(filtered_questions) < 5:\n",
    "        filtered_questions.extend(generated_questions[:5-len(filtered_questions)])\n",
    "    \n",
    "    # Randomly select questions if we have more than needed\n",
    "    if len(filtered_questions) > 20:  # Assuming we want 20 questions in the final paper\n",
    "        filtered_questions = random.sample(filtered_questions, 20)\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_paper(questions):\n",
    "    paper = \"Model Question Paper\\n\\n\"\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        paper += f\"{i}. {question}\\n\\n\"\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pdf(text, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    y = height - 40  # Start near the top of the page\n",
    "    for line in lines:\n",
    "        # If we're near the bottom of the page, start a new page\n",
    "        if y < 40:\n",
    "            c.showPage()\n",
    "            y = height - 40\n",
    "        \n",
    "        c.drawString(40, y, line)\n",
    "        y -= 15  # Move down for the next line\n",
    "    \n",
    "    c.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context, num_questions):\n",
    "    # Load the MiniCPM model and tokenizer from the local path\n",
    "    model_path = \"E:/Dev/Github/MiniCPM/MiniCPM-Llama3-V-2_5\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    # Ensure special tokens are set correctly\n",
    "    if not hasattr(tokenizer, 'im_start_id'):\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': ['<im_start>', '<im_end>']})\n",
    "        tokenizer.im_start_id = tokenizer.convert_tokens_to_ids('<im_start>')\n",
    "        tokenizer.im_end_id = tokenizer.convert_tokens_to_ids('<im_end>')\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    generated_questions = []\n",
    "    for i in range(num_questions):\n",
    "        start_idx = i * 100 % len(context)\n",
    "        input_text = f\"generate question: {context[start_idx:start_idx+200]}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        # MiniCPM specific generation step\n",
    "        outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, num_beams=4, do_sample=True, top_k=50, top_p=0.95)\n",
    "        \n",
    "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {question}\")  # Print each generated question\n",
    "        if question.strip():  # Accept any non-empty string as a question\n",
    "            generated_questions.append(question)\n",
    "    \n",
    "    print(f\"Generated {len(generated_questions)} questions\")\n",
    "    return generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def generate_questions(context, num_questions):\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    generated_questions = []\n",
    "    for i in range(num_questions):\n",
    "        start_idx = i * 100 % len(context)\n",
    "        input_text = f\"generate question: {context[start_idx:start_idx+200]}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, num_beams=4)\n",
    "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_questions.append(question)\n",
    "    \n",
    "    print(f\"Generated {len(generated_questions)} questions\")\n",
    "    return generated_questions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    if not generated_questions:\n",
    "        print(\"No questions to filter.\")\n",
    "        return []\n",
    "    \n",
    "    filtered_questions = generated_questions[:20]  # Just take the first 20 questions without filtering\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pdf_files):\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    text = extract_text_from_pdfs(pdf_files)\n",
    "    print(f\"Extracted {len(text)} characters of text\")\n",
    "    \n",
    "    print(\"Preprocessing text...\")\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    print(f\"Preprocessed into {len(preprocessed_text)} sentences\")\n",
    "    \n",
    "    print(\"Analyzing questions...\")\n",
    "    analyzed_questions = analyze_questions(text)\n",
    "    print(f\"Analyzed {len(analyzed_questions)} questions\")\n",
    "    \n",
    "    print(\"Identifying topics...\")\n",
    "    topics = identify_topics(preprocessed_text)\n",
    "    print(f\"Identified {len(topics)} topics\")\n",
    "    \n",
    "    print(\"Generating questions...\")\n",
    "    generated_questions = generate_questions(' '.join(preprocessed_text), num_questions=50)\n",
    "    \n",
    "    print(\"Filtering and ranking questions...\")\n",
    "    filtered_questions = filter_and_rank_questions(generated_questions, analyzed_questions, topics)\n",
    "    \n",
    "    print(\"Formatting question paper...\")\n",
    "    final_paper = format_question_paper(filtered_questions)\n",
    "    \n",
    "    print(\"Saving question paper as PDF...\")\n",
    "    save_as_pdf(final_paper, \"model_paperGeneratedLocally.pdf\")\n",
    "    \n",
    "    return final_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the main function\n",
    "pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']\n",
    "model_paper = main(pdf_files)\n",
    "print(\"Model paper generation complete. Output saved as 'model_paper.pdf'.\")\n",
    "print(\"\\nGenerated Model Paper:\")\n",
    "print(model_paper)\n",
    "\n",
    "#pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
