{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all libraries And downloaded necessary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import random\n",
    "import re\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Successfully imported all libraries And downloaded necessary data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_files):\n",
    "    text = \"\"\n",
    "    for pdf_file in pdf_files:\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    print(f\"Extracted text sample: {text[:200]}...\")  # Print a sample of the extracted text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove numbering and special characters\n",
    "    text = re.sub(r'\\d+\\.|\\(|\\)', '', text)\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        preprocessed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_questions(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    analyzed_questions = []\n",
    "    for sent in doc.sents:\n",
    "        if sent.text.strip().endswith('?'):\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            pos_tags = [token.pos_ for token in sent]\n",
    "            analyzed_questions.append({\n",
    "                'text': sent.text,\n",
    "                'entities': entities,\n",
    "                'pos_tags': pos_tags\n",
    "            })\n",
    "    \n",
    "    return analyzed_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topics(preprocessed_text):\n",
    "    # Create a dictionary from the preprocessed text\n",
    "    dictionary = corpora.Dictionary([text.split() for text in preprocessed_text])\n",
    "    \n",
    "    # Create a corpus\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in preprocessed_text]\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100)\n",
    "    \n",
    "    topics = lda_model.print_topics()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    filtered_questions = []\n",
    "    for question in generated_questions:\n",
    "        # Check if the question is similar to any original question\n",
    "        if any(question.lower() in orig['text'].lower() for orig in original_questions):\n",
    "            continue\n",
    "        \n",
    "        # Check if the question contains any topic keywords\n",
    "        if any(keyword in question.lower() for topic in topics for keyword in topic[1].split()):\n",
    "            filtered_questions.append(question)\n",
    "    \n",
    "    # If we have fewer than 5 questions, add some generated questions without filtering\n",
    "    if len(filtered_questions) < 5:\n",
    "        filtered_questions.extend(generated_questions[:5-len(filtered_questions)])\n",
    "    \n",
    "    # Randomly select questions if we have more than needed\n",
    "    if len(filtered_questions) > 20:  # Assuming we want 20 questions in the final paper\n",
    "        filtered_questions = random.sample(filtered_questions, 20)\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_paper(questions):\n",
    "    paper = \"Model Question Paper\\n\\n\"\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        paper += f\"{i}. {question}\\n\\n\"\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pdf(text, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    y = height - 40  # Start near the top of the page\n",
    "    for line in lines:\n",
    "        # If we're near the bottom of the page, start a new page\n",
    "        if y < 40:\n",
    "            c.showPage()\n",
    "            y = height - 40\n",
    "        \n",
    "        c.drawString(40, y, line)\n",
    "        y -= 15  # Move down for the next line\n",
    "    \n",
    "    c.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context, num_questions):\n",
    "    # Load the MiniCPM model and tokenizer from the local path\n",
    "    model_path = \"E:/Dev/Github/MiniCPM/MiniCPM-Llama3-V-2_5\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    # Ensure special tokens are set correctly\n",
    "    if not hasattr(tokenizer, 'im_start_id'):\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': ['<im_start>', '<im_end>']})\n",
    "        tokenizer.im_start_id = tokenizer.convert_tokens_to_ids('<im_start>')\n",
    "        tokenizer.im_end_id = tokenizer.convert_tokens_to_ids('<im_end>')\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    generated_questions = []\n",
    "    for i in range(num_questions):\n",
    "        start_idx = i * 100 % len(context)\n",
    "        input_text = f\"generate question: {context[start_idx:start_idx+200]}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        # MiniCPM specific generation step\n",
    "        outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, num_beams=4, do_sample=True, top_k=50, top_p=0.95)\n",
    "        \n",
    "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {question}\")  # Print each generated question\n",
    "        if question.strip():  # Accept any non-empty string as a question\n",
    "            generated_questions.append(question)\n",
    "    \n",
    "    print(f\"Generated {len(generated_questions)} questions\")\n",
    "    return generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_questions(context, num_questions):\\n    model = T5ForConditionalGeneration.from_pretrained(\\'t5-small\\')\\n    tokenizer = T5Tokenizer.from_pretrained(\\'t5-small\\')\\n    \\n    generated_questions = []\\n    for i in range(num_questions):\\n        start_idx = i * 100 % len(context)\\n        input_text = f\"generate question: {context[start_idx:start_idx+200]}\"\\n        input_ids = tokenizer.encode(input_text, return_tensors=\\'pt\\', max_length=512, truncation=True)\\n        \\n        outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, num_beams=4)\\n        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n        generated_questions.append(question)\\n    \\n    print(f\"Generated {len(generated_questions)} questions\")\\n    return generated_questions'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_questions(context, num_questions):\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    generated_questions = []\n",
    "    for i in range(num_questions):\n",
    "        start_idx = i * 100 % len(context)\n",
    "        input_text = f\"generate question: {context[start_idx:start_idx+200]}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, num_beams=4)\n",
    "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_questions.append(question)\n",
    "    \n",
    "    print(f\"Generated {len(generated_questions)} questions\")\n",
    "    return generated_questions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    if not generated_questions:\n",
    "        print(\"No questions to filter.\")\n",
    "        return []\n",
    "    \n",
    "    filtered_questions = generated_questions[:20]  # Just take the first 20 questions without filtering\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pdf_files):\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    text = extract_text_from_pdfs(pdf_files)\n",
    "    print(f\"Extracted {len(text)} characters of text\")\n",
    "    \n",
    "    print(\"Preprocessing text...\")\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    print(f\"Preprocessed into {len(preprocessed_text)} sentences\")\n",
    "    \n",
    "    print(\"Analyzing questions...\")\n",
    "    analyzed_questions = analyze_questions(text)\n",
    "    print(f\"Analyzed {len(analyzed_questions)} questions\")\n",
    "    \n",
    "    print(\"Identifying topics...\")\n",
    "    topics = identify_topics(preprocessed_text)\n",
    "    print(f\"Identified {len(topics)} topics\")\n",
    "    \n",
    "    print(\"Generating questions...\")\n",
    "    generated_questions = generate_questions(' '.join(preprocessed_text), num_questions=50)\n",
    "    \n",
    "    print(\"Filtering and ranking questions...\")\n",
    "    filtered_questions = filter_and_rank_questions(generated_questions, analyzed_questions, topics)\n",
    "    \n",
    "    print(\"Formatting question paper...\")\n",
    "    final_paper = format_question_paper(filtered_questions)\n",
    "    \n",
    "    print(\"Saving question paper as PDF...\")\n",
    "    save_as_pdf(final_paper, \"model_paperGeneratedLocally.pdf\")\n",
    "    \n",
    "    return final_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDFs...\n",
      "Extracted text sample: 1. Explain the differences between data, information,\n",
      "and a database. \n",
      "2. Define the term “Meta Data”. \n",
      "3. Explain why database design is important.\n",
      " \n",
      "4. List and briefly describe the basic building b...\n",
      "Extracted 1186 characters of text\n",
      "Preprocessing text...\n",
      "Preprocessed into 17 sentences\n",
      "Analyzing questions...\n",
      "Analyzed 5 questions\n",
      "Identifying topics...\n",
      "Identified 5 topics\n",
      "Generating questions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cda2a8eab6d4f7ba5355b38cb6b38a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\LENOVO\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-Llama3-V-2_5\\modeling_minicpmv.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids, dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'im_start_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#run the main function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pdf_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial1.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial2.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial3.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m model_paper \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel paper generation complete. Output saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_paper.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Model Paper:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(pdf_files)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentified \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(topics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m topics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating questions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering and ranking questions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m filtered_questions \u001b[38;5;241m=\u001b[39m filter_and_rank_questions(generated_questions, analyzed_questions, topics)\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[1;34m(context, num_questions)\u001b[0m\n\u001b[0;32m     18\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# MiniCPM specific generation step\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m question \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print each generated question\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-Llama3-V-2_5\\modeling_minicpmv.py:333\u001b[0m, in \u001b[0;36mMiniCPMV.generate\u001b[1;34m(self, input_id_list, img_list, tgt_sizes, tokenizer, max_inp_length, vision_hidden_states, return_vision_hidden_states, stream, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     img_list \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bs)]\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bs \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(img_list)\n\u001b[1;32m--> 333\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_id_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_inp_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vision_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-Llama3-V-2_5\\modeling_minicpmv.py:208\u001b[0m, in \u001b[0;36mMiniCPMV._process_list\u001b[1;34m(self, tokenizer, input_id_list, max_inp_length)\u001b[0m\n\u001b[0;32m    205\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m input_id_list:\n\u001b[0;32m    207\u001b[0m     input_tensors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 208\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_inp_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    210\u001b[0m padded \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m pad_keys:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-Llama3-V-2_5\\modeling_minicpmv.py:183\u001b[0m, in \u001b[0;36mMiniCPMV._convert_to_tensors\u001b[1;34m(self, tokenizer, input_ids, max_inp_length)\u001b[0m\n\u001b[0;32m    180\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids[:max_inp_length]\n\u001b[0;32m    181\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_ids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m--> 183\u001b[0m image_start_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim_start_id\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# 跳过 im_start\u001b[39;00m\n\u001b[0;32m    185\u001b[0m image_start_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'im_start_id'"
     ]
    }
   ],
   "source": [
    "#run the main function\n",
    "pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']\n",
    "model_paper = main(pdf_files)\n",
    "print(\"Model paper generation complete. Output saved as 'model_paper.pdf'.\")\n",
    "print(\"\\nGenerated Model Paper:\")\n",
    "print(model_paper)\n",
    "\n",
    "#pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
