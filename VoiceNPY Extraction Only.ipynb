{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_voice_characteristics(audio_file, output_file, sr=22050, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio file and saves them as a .npy file.\n",
    "\n",
    "    Parameters:\n",
    "    audio_file (str): Path to the audio file.\n",
    "    output_file (str): Path to save the .npy file.\n",
    "    sr (int): Sample rate for loading the audio file. Default is 22050.\n",
    "    n_mfcc (int): Number of MFCCs to extract. Default is 13.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Save the MFCC features to a .npy file\n",
    "    np.save(output_file, mfccs)\n",
    "\n",
    "# Example usage\n",
    "audio_file = 'inputs/Rec.wav'\n",
    "output_file = 'voice_characteristics.npy'\n",
    "extract_voice_characteristics(audio_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def extract_voice_characteristics(audio_file, output_file, sr=22050, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio file and saves them as a .npy file.\n",
    "\n",
    "    Parameters:\n",
    "    audio_file (str): Path to the audio file.\n",
    "    output_file (str): Path to save the .npy file.\n",
    "    sr (int): Sample rate for loading the audio file. Default is 22050.\n",
    "    n_mfcc (int): Number of MFCCs to extract. Default is 13.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Save the MFCC features to a .npy file\n",
    "    np.save(output_file, mfccs)\n",
    "\n",
    "    return mfccs  # Return mfccs for inspection\n",
    "\n",
    "# Example usage\n",
    "audio_file = 'inputs/Rec.wav'\n",
    "output_file = 'voice_characteristics2.npy'\n",
    "mfccs = extract_voice_characteristics(audio_file, output_file)\n",
    "\n",
    "# Check the dimensions of mfccs\n",
    "print(\"MFCCs shape:\", mfccs.shape)\n",
    "\n",
    "# Assuming you want to load and inspect as a PyTorch tensor\n",
    "speaker_embeddings = torch.tensor(mfccs)\n",
    "\n",
    "# Check the dimensions of speaker_embeddings\n",
    "print(\"Tensor shape:\", speaker_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.hub\n",
    "\n",
    "# Load the pre-trained speaker embedding model\n",
    "model = torch.hub.load('torchvggish-master', 'vggish')\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'inputs/Rec.wav'  # Replace with your audio file path\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# Preprocess the audio\n",
    "waveform = waveform.mean(dim=0, keepdim=True)  # If stereo, average channels\n",
    "if waveform.size(1) < 16000:\n",
    "    waveform = torch.nn.functional.pad(waveform, (0, 16000 - waveform.size(1)), \"constant\", 0)\n",
    "\n",
    "# Extract speaker embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model.forward(waveform)\n",
    "\n",
    "# Save the embeddings as .npy file\n",
    "output_file = 'speaker_embeddings3.npy'\n",
    "torch.save(embeddings, output_file)\n",
    "\n",
    "print(f\"Speaker embeddings saved to {output_file} with shape {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def extract_voice_characteristics(audio_file, output_file, sr=22050, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio file and saves them as a .npy file.\n",
    "\n",
    "    Parameters:\n",
    "    audio_file (str): Path to the audio file.\n",
    "    output_file (str): Path to save the .npy file.\n",
    "    sr (int): Sample rate for loading the audio file. Default is 22050.\n",
    "    n_mfcc (int): Number of MFCCs to extract. Default is 13.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Pad or truncate mfccs to match torch.Size([1, 512])\n",
    "    mfccs = torch.tensor(mfccs[np.newaxis, :512])  # Adjust to torch tensor and truncate/pad if necessary\n",
    "\n",
    "    # Save the MFCC features to a .npy file\n",
    "    np.save(output_file, mfccs)\n",
    "\n",
    "# Example usage\n",
    "audio_file = 'inputs/Rec.wav'\n",
    "output_file = 'voice_characteristics3.npy'\n",
    "extract_voice_characteristics(audio_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
