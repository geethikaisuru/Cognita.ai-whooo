{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path=\"inputs/cognita_test_lite.pdf\")\n",
    "print(\"Text extraction completed.\")\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "inputs = processor(text=text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOGGLE 1: LOAD ARCTIC DATASET EMBEDDINGS FOR SPEAKER CHARACTERISTICS\n",
    "\n",
    "\"\"\"# load xvector containing speaker's voice characteristics from a dataset\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[10][\"xvector\"]).unsqueeze(0)\n",
    "print(speaker_embeddings.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOGGLE 2: EXTRACT VOICE CHARACTERISTICS FROM  LOCAL AUDIO FILE\n",
    "def extract_voice_characteristics(audio_file, output_file, sr=22050, embedding_dim=512):\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Assuming you have a method to extract a 512-dimensional embedding from the audio\n",
    "    # Replace this with your actual method to get the 512-dimensional embedding\n",
    "    embedding = extract_embedding(y, sr, embedding_dim)\n",
    "\n",
    "    # Convert the embedding to a torch tensor of size [1, 512]\n",
    "    embedding_tensor = torch.tensor(embedding).unsqueeze(0)\n",
    "\n",
    "    # Save the embedding tensor to a .npy file\n",
    "    np.save(output_file, embedding_tensor.numpy())\n",
    "\n",
    "def extract_embedding(y, sr, embedding_dim):\n",
    "    \"\"\"\n",
    "    Placeholder function to extract a 512-dimensional embedding from audio data.\n",
    "    Replace this with your actual method to get the desired embedding.\n",
    "\n",
    "    Parameters:\n",
    "    y (np.ndarray): Audio signal.\n",
    "    sr (int): Sample rate.\n",
    "    embedding_dim (int): Dimensionality of the embedding to extract.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 512-dimensional embedding.\n",
    "    \"\"\"\n",
    "    # Example: Compute MFCCs and flatten to 512 dimensions\n",
    "    n_mfcc = 13  # Example number of MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    embedding = mfccs.flatten()[:embedding_dim]  # Example flattening to 512 dimensions\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOGGLE 2: EXTRACT voice characteristics from the given recording file\n",
    "\n",
    "audio_file = 'inputs/Rec.wav'\n",
    "output_file = 'Voice_npy/voice_characteristics_4.npy'\n",
    "extract_voice_characteristics(audio_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load local voice characteristics from the .npy file\n",
    "local_embeddings = np.load('Voice_npy/voice_characteristics_4.npy')\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "speaker_embeddings = torch.tensor(local_embeddings)\n",
    "\n",
    "# Example to use with the rest of your code\n",
    "print(speaker_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate speech from the text using the speaker characteristics\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "sf.write(\"outputs/speechFromTTSOnly8.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
