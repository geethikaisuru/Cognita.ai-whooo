{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dev\\Projects\\Cognita.ai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mediapipe as mp\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Facial and Upper  Body Detection - MediaPipe eken krnne\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    # Initialize MediaPipe Face Detection and Pose\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lip_sync(video_path, audio_path, output_path):\n",
    "    subprocess.run([\n",
    "        sys.executable, './Wav2Lip/inference.py', '--checkpoint_path', 'checkpoints/wav2lip.pth',\n",
    "        '--face', video_path, '--audio', audio_path, '--outfile', output_path\n",
    "    ], check=True)\n",
    "    print(\"Lip Sync Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hand gesture detection\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Process each frame for hand gestures\n",
    "def detect_hand_gestures(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, audio_path):\n",
    "    processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "\n",
    "    # load xvector containing speaker's voice characteristics from a dataset\n",
    "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "    sf.write(\"speech2.wav\", speech.numpy(), samplerate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path, video_path, output_video_path):\n",
    "    try:\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        print(\"Text extraction completed.\")\n",
    "        \n",
    "        audio_path = \"./train_audio.WAV\"\n",
    "        text_to_speech(text, audio_path)\n",
    "        print(f\"Audio file generated at: {audio_path}\")\n",
    "\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(\"Audio file was not generated. Please check the text_to_speech function.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Text to Speech generation completed.\")\n",
    "        generate_lip_sync(video_path, audio_path, output_video_path)\n",
    "        print(\"Lip sync process completed.\")\n",
    "\n",
    "        if not os.path.exists(output_video_path):\n",
    "            print(\"Output video was not generated. Please check the lip sync process.\")\n",
    "            return\n",
    "\n",
    "        print(\"Processing completed successfully. Check the output video.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction completed.\n",
      "Audio file generated at: ./train_audio.WAV\n",
      "Audio file was not generated. Please check the text_to_speech function.\n",
      "Processing complete. Check the output video.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./cognita_test_lite.pdf\"\n",
    "    video_path = \"./train_vid.mp4\"\n",
    "    output_video_path = \"./output_video.mp4\"\n",
    "\n",
    "    process_pdf(pdf_path, video_path, output_video_path)\n",
    "    print(\"Processing complete. Check the output video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
