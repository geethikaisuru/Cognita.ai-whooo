{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all libraries And downloaded necessary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import random\n",
    "import ollama\n",
    "import re\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Successfully imported all libraries And downloaded necessary data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_files):\n",
    "    text = \"\"\n",
    "    for pdf_file in pdf_files:\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    print(f\"Extracted text sample: {text[:200]}...\")  # Print a sample of the extracted text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove numbering and special characters\n",
    "    text = re.sub(r'\\d+\\.|\\(|\\)', '', text)\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        preprocessed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_questions(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    analyzed_questions = []\n",
    "    for sent in doc.sents:\n",
    "        if sent.text.strip().endswith('?'):\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            pos_tags = [token.pos_ for token in sent]\n",
    "            analyzed_questions.append({\n",
    "                'text': sent.text,\n",
    "                'entities': entities,\n",
    "                'pos_tags': pos_tags\n",
    "            })\n",
    "    \n",
    "    return analyzed_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topics(preprocessed_text):\n",
    "    # Create a dictionary from the preprocessed text\n",
    "    dictionary = corpora.Dictionary([text.split() for text in preprocessed_text])\n",
    "    \n",
    "    # Create a corpus\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in preprocessed_text]\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100)\n",
    "    \n",
    "    topics = lda_model.print_topics()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    filtered_questions = []\n",
    "    for question in generated_questions:\n",
    "        processed_question = post_process_question(question)\n",
    "        if len(processed_question.split()) >= 5 and processed_question not in filtered_questions:\n",
    "            filtered_questions.append(processed_question)\n",
    "    \n",
    "    # Limit to 20 questions\n",
    "    filtered_questions = filtered_questions[:20]\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_paper(questions):\n",
    "    paper = \"Model Question Paper\\n\\n\"\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        paper += f\"{i}. {question}\\n\\n\"\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pdf(text, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    y = height - 40  # Start near the top of the page\n",
    "    for line in lines:\n",
    "        # If we're near the bottom of the page, start a new page\n",
    "        if y < 40:\n",
    "            c.showPage()\n",
    "            y = height - 40\n",
    "        \n",
    "        c.drawString(40, y, line)\n",
    "        y -= 15  # Move down for the next line\n",
    "    \n",
    "    c.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context, num_questions):\n",
    "    generated_questions = []\n",
    "    for i in range(num_questions):\n",
    "        start_idx = i * 100 % len(context)\n",
    "        prompt = f\"Generate a question based on the following context: {context[start_idx:start_idx+200]}\\n\\nQuestion:\"\n",
    "        \n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                                 json={\n",
    "                                     \"model\": \"phi3:mini\",\n",
    "                                     \"prompt\": prompt,\n",
    "                                     \"stream\": False\n",
    "                                 })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            question = response.json()['response'].strip()\n",
    "            generated_questions.append(question)\n",
    "        else:\n",
    "            print(f\"Error generating question: {response.status_code}\")\n",
    "    \n",
    "    print(f\"Generated {len(generated_questions)} questions\")\n",
    "    return generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rank_questions(generated_questions, original_questions, topics):\n",
    "    if not generated_questions:\n",
    "        print(\"No questions to filter.\")\n",
    "        return []\n",
    "    \n",
    "    filtered_questions = generated_questions[:20]  # Just take the first 20 questions without filtering\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_questions)} questions\")\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_question(question):\n",
    "    # Remove any text before the actual question\n",
    "    question = re.sub(r'^.*?([A-Z])', r'\\1', question, flags=re.DOTALL)\n",
    "    \n",
    "    # Capitalize the first letter\n",
    "    question = question.capitalize()\n",
    "    \n",
    "    # Ensure the question ends with a question mark\n",
    "    if not question.endswith('?'):\n",
    "        question += '?'\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pdf_files):\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    text = extract_text_from_pdfs(pdf_files)\n",
    "    print(f\"Extracted {len(text)} characters of text\")\n",
    "    \n",
    "    print(\"Preprocessing text...\")\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    print(f\"Preprocessed into {len(preprocessed_text)} sentences\")\n",
    "    \n",
    "    print(\"Analyzing questions...\")\n",
    "    analyzed_questions = analyze_questions(text)\n",
    "    print(f\"Analyzed {len(analyzed_questions)} questions\")\n",
    "    \n",
    "    print(\"Identifying topics...\")\n",
    "    topics = identify_topics(preprocessed_text)\n",
    "    print(f\"Identified {len(topics)} topics\")\n",
    "    \n",
    "    print(\"Generating questions...\")\n",
    "    generated_questions = generate_questions(' '.join(preprocessed_text), num_questions=50)\n",
    "    \n",
    "    print(\"Filtering and ranking questions...\")\n",
    "    filtered_questions = filter_and_rank_questions(generated_questions, analyzed_questions, topics)\n",
    "    \n",
    "    print(\"Formatting question paper...\")\n",
    "    final_paper = format_question_paper(filtered_questions)\n",
    "    \n",
    "    print(\"Saving question paper as PDF...\")\n",
    "    save_as_pdf(final_paper, \"model_paperGeneratedLocally.pdf\")\n",
    "    \n",
    "    return final_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDFs...\n",
      "Extracted text sample: 1. Explain the differences between data, information,\n",
      "and a database. \n",
      "2. Define the term “Meta Data”. \n",
      "3. Explain why database design is important.\n",
      " \n",
      "4. List and briefly describe the basic building b...\n",
      "Extracted 1186 characters of text\n",
      "Preprocessing text...\n",
      "Preprocessed into 17 sentences\n",
      "Analyzing questions...\n",
      "Analyzed 5 questions\n",
      "Identifying topics...\n",
      "Identified 5 topics\n",
      "Generating questions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#run the main function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pdf_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial1.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial2.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PGenInputs/Papers/DBTutorial3.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m model_paper \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel paper generation complete. Output saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_paper.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Model Paper:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(pdf_files)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentified \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(topics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m topics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating questions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering and ranking questions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m filtered_questions \u001b[38;5;241m=\u001b[39m filter_and_rank_questions(generated_questions, analyzed_questions, topics)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[1;34m(context, num_questions)\u001b[0m\n\u001b[0;32m      4\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(context)\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a question based on the following context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext[start_idx:start_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:11434/api/generate\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m                          json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      9\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3:mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     11\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m                          })\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     15\u001b[0m     question \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "#run the main function\n",
    "pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']\n",
    "model_paper = main(pdf_files)\n",
    "print(\"Model paper generation complete. Output saved as 'model_paper.pdf'.\")\n",
    "print(\"\\nGenerated Model Paper:\")\n",
    "print(model_paper)\n",
    "\n",
    "#pdf_files = ['./PGenInputs/Papers/DBTutorial1.pdf', './PGenInputs/Papers/DBTutorial2.pdf', './PGenInputs/Papers/DBTutorial3.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
